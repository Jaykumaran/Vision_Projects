{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ultralytics -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For cloud instances\n",
    "# !pip uninstall opencv-python opencv-python-headless\n",
    "# !pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers # hf transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from zipfile import ZipFile\n",
    "from ultralytics import YOLO\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trained TrOCR - as of now \n",
    "# !wget -q \"https://www.dropbox.com/s/3t5b01alpw446p4/lpr_ocr_base.zip?dl=1\" -O lpr_ocr_base.zip\n",
    "# # Inference Data\n",
    "# !wget -q \"https://www.dropbox.com/s/clseshw93iinsff/alpr_image_inference_data.zip?dl=1\" -O alpr_image_inference_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip lpr_ocr_base \n",
    "# !unzip alpr_image_inference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('/home/jaykumaran/Vision_Projects/Vision_Projects/ANPR-OCR/YOLO11/runs/detect/yolo11m-license/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a non-downstreamed checkpoint i.e. TrOCR Large Stage 1 rather than printed or handwritten ckpt\n",
    "# trocr_name = \"microsoft/trocr-large-stage1\"\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-stage1')\n",
    "ocr_model = VisionEncoderDecoderModel.from_pretrained('lpr_ocr_base/').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr(image, processor, model, print_tokens = False):\n",
    "    \n",
    "    \"\"\"image: PIL Image,\n",
    "        print_tokens: Whether to print the generated integer tokens or not\n",
    "        \n",
    "        Returns:\n",
    "            generated_text: OCR text string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform ocr on detected and cropped images\n",
    "    pixel_values = processor(image, return_tensors='pt').pixel_values.to(device)\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    if print_tokens:\n",
    "        print(generated_ids)\n",
    "    \n",
    "    generated_text = processor.batch_decode(\n",
    "        generated_ids, skip_special_tokens = True\n",
    "    )[0]\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box(output, frame, processor, ocr_model, print_tokens = False):\n",
    "    frame = np.array(frame[..., ::-1])\n",
    "    line_width = max(round(sum(frame.shape) / 2 * 0.003), 2)\n",
    "    font_thickness = max(line_width - 1, 1)\n",
    "    \n",
    "    for out in output:\n",
    "        for box in out.boxes.xyxy:\n",
    "            point1 = (int(box[0]), int(box[1])) #tuple\n",
    "            point2 = (int(box[2]), int(box[3])) #tuple\n",
    "            \n",
    "            #crop ROI and pass to ocr\n",
    "            license_plate_roi = frame[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "            extracted_text = ocr(license_plate_roi, processor, ocr_model)\n",
    "            \n",
    "            cv2.rectangle(\n",
    "                frame,\n",
    "                point1, point2,\n",
    "                color = (0, 0, 255), # RED\n",
    "                thickness=3\n",
    "            )\n",
    "            \n",
    "            w, h = cv2.getTextSize(\n",
    "                extracted_text,\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale = line_width /3,\n",
    "                thickness=font_thickness\n",
    "            )[0]  #text width and height\n",
    "            \n",
    "            w = int(w - (0.20 * w))\n",
    "            outside = point1[1] - h >= 3 \n",
    "            \n",
    "            point2 = point1[0] + w, point1[1] - h - 3 if outside else point1[1] + h + 3\n",
    "            \n",
    "            cv2.rectangle(\n",
    "                frame,\n",
    "                point1, point2,\n",
    "                color = (0, 0, 255),\n",
    "                thickness=-1,\n",
    "                lineType=cv2.LINE_AA\n",
    "            )\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                extracted_text,\n",
    "                (point1[0], point1[1] - 5 if outside else point1[1] + h + 2),\n",
    "                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                color = (255, 255, 255),\n",
    "                fontScale= line_width / 3.8,\n",
    "                thickness=2,\n",
    "                lineType=cv2.LINE_AA\n",
    "                \n",
    "            )\n",
    "            \n",
    "            \n",
    "        plt.figure(figsize=(30,27))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(frame[..., ::-1])\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_ocr(all_images, processor, ocr_model, print_tokens = False):\n",
    "    \n",
    "    for image_name in all_images:\n",
    "        image = cv2.imread(image_name)[..., ::-1]\n",
    "        output = model.predict(image)\n",
    "        draw_box(output, image, processor, ocr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = glob.glob(os.path.join(\n",
    "    'alpr_image_inference_data', '*'\n",
    "))\n",
    "\n",
    "crop_and_ocr(all_images, processor, ocr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def process_video(video_path, processor, ocr_model, output_path=None):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(\"❌ Error: Video file not found!\")\n",
    "        return\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"❌ Error: Cannot open video file!\")\n",
    "        return\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # ✅ Ensure FPS and dimensions are valid\n",
    "    if fps <= 0 or fps is None:\n",
    "        fps = 30  # Default FPS\n",
    "\n",
    "    if width <= 0 or height <= 0:\n",
    "        print(\"❌ Error: Invalid video dimensions!\")\n",
    "        cap.release()\n",
    "        return\n",
    "\n",
    "    # ✅ Create video writer\n",
    "    out = None\n",
    "    if output_path:\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame is None:\n",
    "            print(\"⚠️ Warning: Empty frame detected, stopping.\")\n",
    "            break  # Stop when video ends or frame is empty\n",
    "        \n",
    "        output = model.predict(frame)\n",
    "        frame = draw_box(output, frame, processor, ocr_model)\n",
    "\n",
    "        # ✅ Ensure frame is valid before writing\n",
    "        if output_path and out is not None and frame is not None and frame.shape[0] > 0 and frame.shape[1] > 0:\n",
    "            out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        else:\n",
    "            print(\"⚠️ Skipping frame: Invalid or empty\")\n",
    "\n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "        print(f\"✅ Video saved successfully at {output_path}\")\n",
    "\n",
    "# Run the video processing\n",
    "video_path = \"/home/jaykumaran/Vision_Projects/ANPR-OCR/TrOCR/mycarplate.mp4\"\n",
    "output_video_path = \"indian_license_output.mp4\"\n",
    "process_video(video_path, processor, ocr_model, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/home/jaykumaran/Vision_Projects/ANPR-OCR/TrOCR/mycarplate.mp4\"\n",
    "output_video_path = \"./\"\n",
    "process_video(video_path, processor, ocr_model, output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
